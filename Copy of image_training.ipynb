{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1rmnIuhQ_6kqqamBKr-TgF3C-aLDJlo_k","timestamp":1746469568629}],"authorship_tag":"ABX9TyOu3ao4vZLpj4a+lOpuNIqU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XcIpQ0zQ0IVW","executionInfo":{"status":"ok","timestamp":1746469199030,"user_tz":420,"elapsed":25136,"user":{"displayName":"Jiacheng Sun","userId":"00245126575781883405"}},"outputId":"7434ecd1-8979-48a0-afa9-e23140be38c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import random\n","from PIL import Image\n","from torchvision import transforms, models\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Paths\n","root_dir = \"/content/drive/MyDrive/traffic data\"\n","folders = [\n","    \"image2andmore\",\n","    \"image_random_intersection\",\n","    \"images_oneAccident_minor\",\n","    \"images_oneAccident_possible\",\n","    \"images_oneAccident_serious\"\n","]\n","metadata_path = os.path.join(root_dir, \"combined_metadata.csv\")\n","\n","# Load metadata\n","metadata = pd.read_csv(metadata_path)\n","metadata['image_filename'] = metadata['image_filename'].astype(str)\n","\n","# Combine image paths\n","image_paths = []\n","labels = []\n","\n","for folder in folders:\n","    full_path = os.path.join(root_dir, folder)\n","    images = [f for f in os.listdir(full_path) if f.lower().endswith(('.jpg', '.png'))]\n","    selected_images = random.sample(images, min(100, len(images)))  # Pick 100 or all if less\n","\n","    for img_name in selected_images:\n","        label_row = metadata[metadata['image_filename'] == img_name]\n","        if not label_row.empty:\n","            risk_label = int(label_row['risk'].values[0])\n","            image_paths.append(os.path.join(full_path, img_name))\n","            labels.append(risk_label)\n","\n","# Dataset class\n","class RoadRiskDataset(Dataset):\n","    def __init__(self, image_paths, labels, transform=None):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.transform = transform or transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n","        img = self.transform(img)\n","        label = self.labels[idx]\n","        return img, label\n","\n","# Dataset and DataLoader\n","dataset = RoadRiskDataset(image_paths, labels)\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n","\n","\n"],"metadata":{"id":"CP-b64Mo3rVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load pretrained ResNet-18\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = models.resnet18(pretrained=True)\n","model.fc = nn.Linear(model.fc.in_features, 5)  # 5 classes: risk 0‚Äì4\n","model = model.to(device)\n","\n","# Example training step (optional)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# One batch training loop (preview)\n","model.train()\n","for imgs, targets in dataloader:\n","    imgs, targets = imgs.to(device), targets.to(device)\n","    outputs = model(imgs)\n","    loss = criterion(outputs, targets)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    print(\"Sample training loss:\", loss.item())\n","    break  # Remove this to train fully\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hZ753Nn5KF1","executionInfo":{"status":"ok","timestamp":1745365895258,"user_tz":420,"elapsed":20363,"user":{"displayName":"Jiacheng Sun","userId":"00245126575781883405"}},"outputId":"50fdcfc6-f035-43ff-807c-6af114300b75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 159MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Sample training loss: 1.7444162368774414\n"]}]},{"cell_type":"code","source":["import random\n","from torch.utils.data import Subset\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Select 50 random indices\n","random_indices = random.sample(range(len(dataloader.dataset)), 50)\n","\n","# Create a subset dataset\n","subset_dataset = Subset(dataloader.dataset, random_indices)\n","subset_loader = DataLoader(subset_dataset, batch_size=16, shuffle=False)\n","\n","# Evaluation\n","model.eval()\n","all_preds = []\n","all_targets = []\n","\n","with torch.no_grad():\n","    for imgs, targets in subset_loader:\n","        imgs = imgs.to(device)\n","        targets = targets.to(device)\n","        outputs = model(imgs)\n","        preds = torch.argmax(outputs, dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_targets.extend(targets.cpu().numpy())\n","\n","# Metrics\n","acc = accuracy_score(all_targets, all_preds)\n","print(f\"\\n‚úÖ Accuracy on 50 Random Images: {acc:.4f}\")\n","\n","print(\"\\nüìä Classification Report:\")\n","print(classification_report(all_targets, all_preds, digits=3))\n","\n","print(\"\\nüåÄ Confusion Matrix:\")\n","print(confusion_matrix(all_targets, all_preds))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VhPm9_6K6h3z","executionInfo":{"status":"ok","timestamp":1745366227866,"user_tz":420,"elapsed":16295,"user":{"displayName":"Jiacheng Sun","userId":"00245126575781883405"}},"outputId":"7831f2f3-1a0b-47a1-e83d-5d4398752dc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Accuracy on 50 Random Images: 0.2200\n","\n","üìä Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0      1.000     0.100     0.182        10\n","           1      0.200     0.100     0.133        10\n","           2      0.000     0.000     0.000         8\n","           3      0.174     0.400     0.242        10\n","           4      0.238     0.417     0.303        12\n","\n","    accuracy                          0.220        50\n","   macro avg      0.322     0.203     0.172        50\n","weighted avg      0.332     0.220     0.184        50\n","\n","\n","üåÄ Confusion Matrix:\n","[[1 1 0 3 5]\n"," [0 1 0 3 6]\n"," [0 1 0 6 1]\n"," [0 2 0 4 4]\n"," [0 0 0 7 5]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]},{"cell_type":"code","source":["import random\n","from torch.utils.data import Subset\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","\n","random_indices = random.sample(range(len(dataloader.dataset)), 300)\n","\n","# Create a subset dataset\n","subset_dataset = Subset(dataloader.dataset, random_indices)\n","subset_loader = DataLoader(subset_dataset, batch_size=16, shuffle=False)\n","\n","# Evaluation\n","model.eval()\n","all_preds = []\n","all_targets = []\n","\n","with torch.no_grad():\n","    for imgs, targets in subset_loader:\n","        imgs = imgs.to(device)\n","        targets = targets.to(device)\n","        outputs = model(imgs)\n","        preds = torch.argmax(outputs, dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_targets.extend(targets.cpu().numpy())\n","\n","# Metrics\n","acc = accuracy_score(all_targets, all_preds)\n","print(f\"\\n‚úÖ Accuracy on 50 Random Images: {acc:.4f}\")\n","\n","print(\"\\nüìä Classification Report:\")\n","print(classification_report(all_targets, all_preds, digits=3))\n","\n","print(\"\\nüåÄ Confusion Matrix:\")\n","print(confusion_matrix(all_targets, all_preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yDZzfjda8XGM","executionInfo":{"status":"ok","timestamp":1745366511938,"user_tz":420,"elapsed":112562,"user":{"displayName":"Jiacheng Sun","userId":"00245126575781883405"}},"outputId":"ac397563-fa2f-4fc8-f9a7-dbfa00b3df12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Accuracy on 50 Random Images: 0.2033\n","\n","üìä Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0      1.000     0.076     0.141        66\n","           1      0.310     0.148     0.200        61\n","           2      0.500     0.016     0.031        63\n","           3      0.163     0.500     0.246        48\n","           4      0.188     0.355     0.246        62\n","\n","    accuracy                          0.203       300\n","   macro avg      0.432     0.219     0.173       300\n","weighted avg      0.453     0.203     0.168       300\n","\n","\n","üåÄ Confusion Matrix:\n","[[ 5  6  1 20 34]\n"," [ 0  9  0 32 20]\n"," [ 0  6  1 35 21]\n"," [ 0  4  0 24 20]\n"," [ 0  4  0 36 22]]\n"]}]},{"cell_type":"code","source":["import random\n","from torch.utils.data import Subset\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Select 50 random indices\n","random_indices = random.sample(range(len(dataloader.dataset)), 50)\n","\n","# Create a subset dataset\n","subset_dataset = Subset(dataloader.dataset, random_indices)\n","subset_loader = DataLoader(subset_dataset, batch_size=16, shuffle=False)\n","\n","# Evaluation\n","model.eval()\n","all_preds = []\n","all_targets = []\n","\n","with torch.no_grad():\n","    for imgs, targets in subset_loader:\n","        imgs = imgs.to(device)\n","        targets = targets.to(device)\n","        outputs = model(imgs)\n","        preds = torch.argmax(outputs, dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_targets.extend(targets.cpu().numpy())\n","\n","# Metrics\n","acc = accuracy_score(all_targets, all_preds)\n","print(f\"\\n‚úÖ Accuracy on 50 Random Images: {acc:.4f}\")\n","\n","print(\"\\nüìä Classification Report:\")\n","print(classification_report(all_targets, all_preds, digits=3))\n","\n","print(\"\\nüåÄ Confusion Matrix:\")\n","print(confusion_matrix(all_targets, all_preds))"],"metadata":{"id":"GHMV6qEU8kQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import random\n","from PIL import Image\n","from torchvision import transforms, models\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Paths\n","root_dir = \"/content/drive/MyDrive/traffic data\"\n","folders = [\n","    \"image2andmore\",\n","    \"image_random_intersection\",\n","    \"images_oneAccident_minor\",\n","    \"images_oneAccident_possible\",\n","    \"images_oneAccident_serious\"\n","]\n","metadata_path = os.path.join(root_dir, \"combined_metadata.csv\")\n","\n","# Load metadata\n","metadata = pd.read_csv(metadata_path)\n","metadata['image_filename'] = metadata['image_filename'].astype(str)\n","\n","# Combine image paths\n","image_paths = []\n","labels = []\n","\n","for folder in folders:\n","    full_path = os.path.join(root_dir, folder)\n","    images = [f for f in os.listdir(full_path) if f.lower().endswith(('.jpg', '.png'))]\n","    selected_images = random.sample(images, min(1000, len(images)))  # Pick 100 or all if less\n","\n","    for img_name in selected_images:\n","        label_row = metadata[metadata['image_filename'] == img_name]\n","        if not label_row.empty:\n","            risk_label = int(label_row['risk'].values[0])\n","            image_paths.append(os.path.join(full_path, img_name))\n","            labels.append(risk_label)\n","\n","# Dataset class\n","class RoadRiskDataset(Dataset):\n","    def __init__(self, image_paths, labels, transform=None):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.transform = transform or transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n","        img = self.transform(img)\n","        label = self.labels[idx]\n","        return img, label\n","\n","# Dataset and DataLoader\n","dataset = RoadRiskDataset(image_paths, labels)\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n","\n","\n"],"metadata":{"id":"Pwc3B8OR9DeH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load pretrained ResNet-18\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = models.resnet18(pretrained=True)\n","model.fc = nn.Linear(model.fc.in_features, 5)  # 5 classes: risk 0‚Äì4\n","model = model.to(device)\n","\n","# Example training step (optional)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# One batch training loop (preview)\n","model.train()\n","for imgs, targets in dataloader:\n","    imgs, targets = imgs.to(device), targets.to(device)\n","    outputs = model(imgs)\n","    loss = criterion(outputs, targets)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    print(\"Sample training loss:\", loss.item())\n","    break  # Remove this to train fully\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5WUIBK49QJw","executionInfo":{"status":"ok","timestamp":1745366645290,"user_tz":420,"elapsed":21218,"user":{"displayName":"Jiacheng Sun","userId":"00245126575781883405"}},"outputId":"d48085aa-121f-48a1-8094-b3341f5e462a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Sample training loss: 1.5364779233932495\n"]}]},{"cell_type":"code","source":["import random\n","from torch.utils.data import Subset\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Select 50 random indices\n","random_indices = random.sample(range(len(dataloader.dataset)), 100)\n","\n","# Create a subset dataset\n","subset_dataset = Subset(dataloader.dataset, random_indices)\n","subset_loader = DataLoader(subset_dataset, batch_size=16, shuffle=False)\n","\n","# Evaluation\n","model.eval()\n","all_preds = []\n","all_targets = []\n","\n","with torch.no_grad():\n","    for imgs, targets in subset_loader:\n","        imgs = imgs.to(device)\n","        targets = targets.to(device)\n","        outputs = model(imgs)\n","        preds = torch.argmax(outputs, dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_targets.extend(targets.cpu().numpy())\n","\n","# Metrics\n","acc = accuracy_score(all_targets, all_preds)\n","print(f\"\\n‚úÖ Accuracy on 50 Random Images: {acc:.4f}\")\n","\n","print(\"\\nüìä Classification Report:\")\n","print(classification_report(all_targets, all_preds, digits=3))\n","\n","print(\"\\nüåÄ Confusion Matrix:\")\n","print(confusion_matrix(all_targets, all_preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I6dNvmZy9c9M","executionInfo":{"status":"ok","timestamp":1745366753434,"user_tz":420,"elapsed":76966,"user":{"displayName":"Jiacheng Sun","userId":"00245126575781883405"}},"outputId":"7fae2386-a875-456d-c2ac-de04d322c1e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Accuracy on 50 Random Images: 0.2200\n","\n","üìä Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0      0.235     0.364     0.286        22\n","           1      0.500     0.100     0.167        20\n","           2      0.148     0.444     0.222        18\n","           3      0.500     0.050     0.091        20\n","           4      0.500     0.150     0.231        20\n","\n","    accuracy                          0.220       100\n","   macro avg      0.377     0.222     0.199       100\n","weighted avg      0.378     0.220     0.201       100\n","\n","\n","üåÄ Confusion Matrix:\n","[[ 8  2 12  0  0]\n"," [ 2  2 15  0  1]\n"," [ 8  0  8  0  2]\n"," [11  0  8  1  0]\n"," [ 5  0 11  1  3]]\n"]}]},{"cell_type":"code","source":["# Calculate average absolute difference between predicted and actual risk\n","differences = [abs(p - t) for p, t in zip(all_preds, all_targets)]\n","avg_difference = sum(differences) / len(differences)\n","\n","print(f\"\\nüìè Average Absolute Risk Difference: {avg_difference:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1b-rO8gG_19b","executionInfo":{"status":"ok","timestamp":1745367306763,"user_tz":420,"elapsed":7,"user":{"displayName":"Jiacheng Sun","userId":"00245126575781883405"}},"outputId":"6f1e0ba7-5a26-49fa-c2a7-272d4f85668c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üìè Average Absolute Risk Difference: 1.500\n"]}]},{"cell_type":"code","source":["import random\n","from torch.utils.data import Subset, DataLoader\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Select 200 random indices from dataset\n","random_indices = random.sample(range(len(dataloader.dataset)), 200)\n","\n","# Create subset and loader\n","subset_dataset = Subset(dataloader.dataset, random_indices)\n","subset_loader = DataLoader(subset_dataset, batch_size=16, shuffle=False)\n","\n","# Evaluate\n","model.eval()\n","all_preds = []\n","all_targets = []\n","\n","with torch.no_grad():\n","    for imgs, targets in subset_loader:\n","        imgs = imgs.to(device)\n","        targets = targets.to(device)\n","        outputs = model(imgs)\n","        preds = torch.argmax(outputs, dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_targets.extend(targets.cpu().numpy())\n","\n","# üéØ Map to 3 custom groups:\n","def map_to_group(x):\n","    if x == 0:\n","        return 0  # Low\n","    elif x in [1, 2, 3]:\n","        return 1  # Medium\n","    else:\n","        return 2  # High\n","\n","grouped_preds = [map_to_group(p) for p in all_preds]\n","grouped_targets = [map_to_group(t) for t in all_targets]\n","\n","# üîç Metrics\n","acc = accuracy_score(grouped_targets, grouped_preds)\n","print(f\"\\n‚úÖ Accuracy (3-Group Risk Classification): {acc:.4f}\")\n","\n","print(\"\\nüìä Classification Report (3 Groups):\")\n","print(classification_report(grouped_targets, grouped_preds, target_names=[\"Low\", \"Medium\", \"High\"], digits=3))\n","\n","print(\"\\nüåÄ Confusion Matrix (3 Groups):\")\n","print(confusion_matrix(grouped_targets, grouped_preds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sy4zR8jLBCnS","executionInfo":{"status":"ok","timestamp":1745367767759,"user_tz":420,"elapsed":157066,"user":{"displayName":"Jiacheng Sun","userId":"00245126575781883405"}},"outputId":"786ad133-0662-418c-8b9a-4440b279242a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Accuracy (3-Group Risk Classification): 0.4500\n","\n","üìä Classification Report (3 Groups):\n","              precision    recall  f1-score   support\n","\n","         Low      0.280     0.548     0.371        42\n","      Medium      0.580     0.570     0.575       114\n","        High      0.333     0.045     0.080        44\n","\n","    accuracy                          0.450       200\n","   macro avg      0.398     0.388     0.342       200\n","weighted avg      0.463     0.450     0.423       200\n","\n","\n","üåÄ Confusion Matrix (3 Groups):\n","[[23 19  0]\n"," [45 65  4]\n"," [14 28  2]]\n"]}]},{"cell_type":"code","source":["from torch.utils.data import Subset\n","import random\n","\n","# First, index dataset samples by their group\n","low_idxs = []\n","medium_idxs = []\n","high_idxs = []\n","\n","for i in range(len(dataset)):\n","    _, label = dataset[i]  # get only the label\n","    if label == 0:\n","        low_idxs.append(i)\n","    elif label in [1, 2, 3]:\n","        medium_idxs.append(i)\n","    elif label == 4:\n","        high_idxs.append(i)\n","\n","# Choose how many samples you want per group\n","n_per_group = 100  # or adjust as needed\n","\n","# Sample from each group\n","balanced_test_indices = (\n","    random.sample(low_idxs, min(n_per_group, len(low_idxs))) +\n","    random.sample(medium_idxs, min(n_per_group, len(medium_idxs))) +\n","    random.sample(high_idxs, min(n_per_group, len(high_idxs)))\n",")\n","\n","# Optional: shuffle the final index list\n","random.shuffle(balanced_test_indices)\n","\n","# Create balanced test set and loader\n","balanced_test_dataset = Subset(dataset, balanced_test_indices)\n","balanced_test_loader = DataLoader(balanced_test_dataset, batch_size=16, shuffle=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"id":"YllhVff2EHQx","executionInfo":{"status":"error","timestamp":1745369290481,"user_tz":420,"elapsed":872793,"user":{"displayName":"Jiacheng Sun","userId":"00245126575781883405"}},"outputId":"b6a04f24-f759-4cdb-f316-fd1aba849c31"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-852fade2849c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# get only the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlow_idxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-6cc2e11bc453>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3474\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3476\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import numpy as np\n","from torch.utils.data import Subset\n","# Configuration\n","n_per_class = 40  # 40 samples per class √ó 5 classes = 200 total test samples\n","total_test_samples = n_per_class * 5\n","\n","# Pre-allocate lists for each class\n","class_indices = {cls: [] for cls in range(5)}\n","\n","# Single pass through dataset (optimized)\n","for idx in np.random.permutation(len(dataset))[:1000]:  # Check max 1000 random samples\n","    _, label = dataset[idx]\n","    if len(class_indices[label]) < n_per_class:\n","        class_indices[label].append(idx)\n","    # Early exit if all classes have enough samples\n","    if all(len(v) >= n_per_class for v in class_indices.values()):\n","        break\n","\n","# Combine selected indices\n","selected_indices = []\n","for cls in range(5):\n","    selected_indices.extend(class_indices[cls][:n_per_class])\n","\n","# Create subset and loader\n","subset_dataset = Subset(dataset, selected_indices)\n","subset_loader = DataLoader(subset_dataset, batch_size=16, shuffle=False)\n","\n","# The rest of your evaluation code remains the same...\n","model.eval()\n","all_preds = []\n","all_targets = []\n","\n","with torch.no_grad():\n","    for imgs, targets in subset_loader:\n","        imgs = imgs.to(device)\n","        targets = targets.to(device)\n","        outputs = model(imgs)\n","        preds = torch.argmax(outputs, dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_targets.extend(targets.cpu().numpy())\n","\n","# üéØ Map to 3 custom groups:\n","def map_to_group(x):\n","    if x == 0:\n","        return 0  # Low\n","    elif x in [1, 2, 3]:\n","        return 1  # Medium\n","    else:\n","        return 2  # High\n","\n","grouped_preds = [map_to_group(p) for p in all_preds]\n","grouped_targets = [map_to_group(t) for t in all_targets]\n","\n","# üîç Metrics\n","acc = accuracy_score(grouped_targets, grouped_preds)\n","print(f\"\\n‚úÖ Accuracy (3-Group Risk Classification): {acc:.4f}\")\n","\n","print(\"\\nüìä Classification Report (3 Groups):\")\n","print(classification_report(grouped_targets, grouped_preds, target_names=[\"Low\", \"Medium\", \"High\"], digits=3))\n","\n","print(\"\\nüåÄ Confusion Matrix (3 Groups):\")\n","print(confusion_matrix(grouped_targets, grouped_preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4xGu_hNIQyV","executionInfo":{"status":"ok","timestamp":1745370081776,"user_tz":420,"elapsed":120327,"user":{"displayName":"Jiacheng Sun","userId":"00245126575781883405"}},"outputId":"5524e1fb-52ac-4a00-9f14-c65986eddd39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Accuracy (3-Group Risk Classification): 0.4150\n","\n","üìä Classification Report (3 Groups):\n","              precision    recall  f1-score   support\n","\n","         Low      0.247     0.500     0.331        40\n","      Medium      0.549     0.517     0.532       120\n","        High      0.167     0.025     0.043        40\n","\n","    accuracy                          0.415       200\n","   macro avg      0.321     0.347     0.302       200\n","weighted avg      0.412     0.415     0.394       200\n","\n","\n","üåÄ Confusion Matrix (3 Groups):\n","[[20 20  0]\n"," [53 62  5]\n"," [ 8 31  1]]\n"]}]},{"cell_type":"code","source":["model.eval()\n","all_preds = []\n","all_targets = []\n","\n","with torch.no_grad():\n","    for imgs, targets in balanced_test_loader:\n","        imgs = imgs.to(device)\n","        targets = targets.to(device)\n","        outputs = model(imgs)\n","        preds = torch.argmax(outputs, dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_targets.extend(targets.cpu().numpy())\n"],"metadata":{"id":"vMApCR7CEOPn"},"execution_count":null,"outputs":[]}]}